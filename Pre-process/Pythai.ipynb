{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "%reload_ext autoreload\r\n",
    "%autoreload 2\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "try:\r\n",
    "    %tensorflow_version 2.x\r\n",
    "except:\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import pythainlp\r\n",
    "from pythainlp import word_tokenize\r\n",
    "# from pythainlp.corpus import thai_stopwords as stopwords\r\n",
    "from pythainlp.corpus import wordnet\r\n",
    "from nltk.stem.porter import PorterStemmer\r\n",
    "from nltk.corpus import words\r\n",
    "from stop_words import get_stop_words\r\n",
    "\r\n",
    "pythainlp.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import nltk\r\n",
    "nltk.download('words')\r\n",
    "th_stop = tuple(pythainlp.corpus.thai_stopwords())\r\n",
    "en_stop = tuple(get_stop_words('en'))\r\n",
    "p_stemmer = PorterStemmer()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "clean_text = [\"เผยผลฉีดวัคซีน คนสูงอายุโดยเฉพาะผู้ที่มีโรคร่วม ภูมิคุ้มกันบกพร่อง หรืออยู่บ้านพักคนชรา ภูมิจะลดลงเร็วกว่ากลุ่มอื่น\",\"เผยผลฉีดวัคซีน คนสูงอายุโดยเฉพาะผู้ที่มีโรคร่วม\",\"เผยแพร่ข้อมูลชุดของตนเองบ้างใน preprint พบว่าอัตราการลดลงของประสิทธิผลของวัคซีนที่ใช้ในอังกฤษลดลงทั้ง 3 ชนิด โดยที่ AstraZeneca และ Pfizer vaccine ก็ลดลงในอัตราส่วนพอๆ กันแทบจะเป็นเส้นขนาน เมื่อติดตามไปอย่างน้อย 5 เดือน ประสิทธิผลของวัคซีน Pfizer ยังคงสูงกว่า AstraZenecaอัตราลดลงในกลุ่มเปราะบาง คือคนสูงอายุโดยเฉพาะผู้ที่มีโรคร่วม ภูมิคุ้มกันบกพร่อง หรืออยู่บ้านพักคนชรา จะลดลงเร็วกว่ากลุ่มอื่น ข้อมูลนี้น่าจะเป็นเหตุผลหนึ่งที่ JCVI แนะนำให้เริ่มฉีดวัคซีนเข็มสามในกลุ่มเสี่ยงเหล่านี้ก่อน หลังได้รับวัคซีนครบอย่างน้อย 6 เดือน โดยใช้ Pfizer vaccine เป็นวัคซีนหลัก\"]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def split_word(text):\r\n",
    "            \r\n",
    "    \r\n",
    "    tokens = word_tokenize(text,engine='newmm')\r\n",
    "    \r\n",
    "    # Remove stop words ภาษาไทย และภาษาอังกฤษ\r\n",
    "    tokens = [i for i in tokens if not i in th_stop and not i in en_stop]\r\n",
    "    \r\n",
    "    # หารากศัพท์ภาษาอังกฤษ\r\n",
    "    # English\r\n",
    "    tokens = [p_stemmer.stem(i) for i in tokens]\r\n",
    "    \r\n",
    "    # Thai\r\n",
    "    # tokens_temp=[]\r\n",
    "    # for i in tokens:\r\n",
    "    #     w_syn = wordnet.synsets(i)\r\n",
    "    #     if (len(w_syn)>0) and (len(w_syn[0].lemma_names('tha'))>0):\r\n",
    "    #         tokens_temp.append(w_syn[0].lemma_names('tha')[0])\r\n",
    "    #     else:\r\n",
    "    #         tokens_temp.append(i)\r\n",
    "    \r\n",
    "    # tokens = tokens_temp\r\n",
    "    \r\n",
    "    # ลบตัวเลข\r\n",
    "    tokens = [i for i in tokens if not i.isnumeric()]\r\n",
    "    \r\n",
    "    # ลบช่องว่าง\r\n",
    "    tokens = [i for i in tokens if not ' ' in i]\r\n",
    "\r\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "print('tokenized text:\\n',split_word(clean_text[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenized text:\n",
      " ['เผย', 'ฉีดวัคซีน', 'คนสูงอายุ', 'โดยเฉพาะ', 'โรค', 'ภูมิคุ้มกัน', 'บกพร่อง', 'บ้านพัก', 'คนชรา', 'ภูมิ', 'ลดลง']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "tokens_list = [split_word(txt) for txt in clean_text] # เก็บคำที่แบ่งแล้วเอาไว้ใน tokens_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "tokens_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['เผย',\n",
       "  'ฉีดวัคซีน',\n",
       "  'คนสูงอายุ',\n",
       "  'โดยเฉพาะ',\n",
       "  'โรค',\n",
       "  'ภูมิคุ้มกัน',\n",
       "  'บกพร่อง',\n",
       "  'บ้านพัก',\n",
       "  'คนชรา',\n",
       "  'ภูมิ',\n",
       "  'ลดลง'],\n",
       " ['เผย', 'ฉีดวัคซีน', 'คนสูงอายุ', 'โดยเฉพาะ', 'โรค'],\n",
       " ['เผยแพร่',\n",
       "  'ข้อมูล',\n",
       "  'ชุด',\n",
       "  'preprint',\n",
       "  'อัตรา',\n",
       "  'ลดลง',\n",
       "  'ประสิทธิผล',\n",
       "  'วัคซีน',\n",
       "  'อังกฤษ',\n",
       "  'ลดลง',\n",
       "  'ชนิด',\n",
       "  'โดยที่',\n",
       "  'astrazeneca',\n",
       "  'pfizer',\n",
       "  'vaccin',\n",
       "  'ลดลง',\n",
       "  'อัตราส่วน',\n",
       "  'แทบจะ',\n",
       "  'เส้นขนาน',\n",
       "  'ติดตาม',\n",
       "  'เดือน',\n",
       "  'ประสิทธิผล',\n",
       "  'วัคซีน',\n",
       "  'pfizer',\n",
       "  'astrazeneca',\n",
       "  'อัตรา',\n",
       "  'ลดลง',\n",
       "  'เปราะบาง',\n",
       "  'คนสูงอายุ',\n",
       "  'โดยเฉพาะ',\n",
       "  'โรค',\n",
       "  'ภูมิคุ้มกัน',\n",
       "  'บกพร่อง',\n",
       "  'บ้านพัก',\n",
       "  'คนชรา',\n",
       "  'ลดลง',\n",
       "  'ข้อมูล',\n",
       "  'น่าจะเป็น',\n",
       "  'jcvi',\n",
       "  'แนะนำ',\n",
       "  'ฉีดวัคซีน',\n",
       "  'เข็ม',\n",
       "  'สาม',\n",
       "  'เสี่ยง',\n",
       "  'วัคซีน',\n",
       "  'เดือน',\n",
       "  'pfizer',\n",
       "  'vaccin',\n",
       "  'วัคซีน',\n",
       "  'หลัก']]"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "tokens_list_j = [','.join(tkn) for tkn in tokens_list] \r\n",
    "cvec = CountVectorizer(analyzer=lambda x:x.split(','))\r\n",
    "c_feat = cvec.fit_transform(tokens_list_j) #วิธีที่ 1 ทำ Bag of words + count word"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "cvec.vocabulary_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'เผย': 28,\n",
       " 'ฉีดวัคซีน': 8,\n",
       " 'คนสูงอายุ': 7,\n",
       " 'โดยเฉพาะ': 35,\n",
       " 'โรค': 36,\n",
       " 'ภูมิคุ้มกัน': 17,\n",
       " 'บกพร่อง': 13,\n",
       " 'บ้านพัก': 14,\n",
       " 'คนชรา': 6,\n",
       " 'ภูมิ': 16,\n",
       " 'ลดลง': 18,\n",
       " 'เผยแพร่': 29,\n",
       " 'ข้อมูล': 5,\n",
       " 'ชุด': 10,\n",
       " 'preprint': 3,\n",
       " 'อัตรา': 23,\n",
       " 'ประสิทธิผล': 15,\n",
       " 'วัคซีน': 19,\n",
       " 'อังกฤษ': 22,\n",
       " 'ชนิด': 9,\n",
       " 'โดยที่': 34,\n",
       " 'astrazeneca': 0,\n",
       " 'pfizer': 2,\n",
       " 'vaccin': 4,\n",
       " 'อัตราส่วน': 24,\n",
       " 'แทบจะ': 32,\n",
       " 'เส้นขนาน': 31,\n",
       " 'ติดตาม': 11,\n",
       " 'เดือน': 26,\n",
       " 'เปราะบาง': 27,\n",
       " 'น่าจะเป็น': 12,\n",
       " 'jcvi': 1,\n",
       " 'แนะนำ': 33,\n",
       " 'เข็ม': 25,\n",
       " 'สาม': 20,\n",
       " 'เสี่ยง': 30,\n",
       " 'หลัก': 21}"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "c_feat[:,:].todense()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [2, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 5, 4, 1,\n",
       "         1, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "tvec = TfidfVectorizer(analyzer=lambda x:x.split(','),)\r\n",
    "t_feat = tvec.fit_transform(tokens_list_j) # วิธีที่ 2 ทำ Bag of words + tf-idf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "t_feat[:,:].todense()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.31401745, 0.24386256, 0.24386256, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.31401745, 0.31401745,\n",
       "         0.        , 0.41289521, 0.31401745, 0.31401745, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.31401745, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.24386256, 0.24386256],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.42040099, 0.42040099, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.54134281, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.42040099, 0.42040099],\n",
       "        [0.21671526, 0.10835763, 0.32507289, 0.10835763, 0.21671526,\n",
       "         0.21671526, 0.08240877, 0.06399776, 0.06399776, 0.10835763,\n",
       "         0.10835763, 0.10835763, 0.10835763, 0.08240877, 0.08240877,\n",
       "         0.21671526, 0.        , 0.08240877, 0.41204385, 0.43343052,\n",
       "         0.10835763, 0.10835763, 0.10835763, 0.21671526, 0.10835763,\n",
       "         0.10835763, 0.21671526, 0.10835763, 0.        , 0.10835763,\n",
       "         0.10835763, 0.10835763, 0.10835763, 0.10835763, 0.10835763,\n",
       "         0.06399776, 0.06399776]])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(tvec.idf_),len(tvec.vocabulary_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "37 37\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "11f1dc213e07634baa4c5c321dec03c05dafae643c50f20e6d1a492290c05dc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}